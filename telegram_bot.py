import os
import json
import logging
import threading
import traceback
import requests
import random
import asyncio
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup, InputMediaPhoto, BotCommand, BotCommandScopeChat, BotCommandScopeDefault
from telegram.constants import ParseMode
from telegram.ext import Application, CommandHandler, CallbackQueryHandler, MessageHandler, filters, ContextTypes
from bs4 import BeautifulSoup
import supabase_utils
from dotenv import load_dotenv
from io import BytesIO
from PIL import Image

load_dotenv()

TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
ADMIN_USER_ID = os.getenv("TELEGRAM_ADMIN_ID")
SUPABASE_BUCKET = "monitor-data"
USERS_FILE = "bot_users.json"
CODES_FILE = "active_codes.json"
POLL_INTERVAL = 120
MAX_JOB_RUNTIME = 110
POTENTIAL_USERS_FILE = "potential_users.json"
broadcast_lock = asyncio.Lock()
job_start_time = None

sync_executor = ThreadPoolExecutor(max_workers=5, thread_name_prefix="sync_io")

logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logging.getLogger("httpx").setLevel(logging.WARNING)
logger = logging.getLogger(__name__)

# --- DATETIME PARSING UTILITY ---

def parse_iso_datetime(iso_string: str) -> datetime:
    """Parse ISO format datetime string with flexible microseconds."""
    if not iso_string:
        return datetime.utcnow()
    
    try:
        return datetime.fromisoformat(iso_string)
    except ValueError:
        try:
            if "." in iso_string:
                parts = iso_string.split(".")
                if "+" in parts[1]:
                    ms, tz = parts[1].split("+")
                    ms = (ms + "000000")[:6]
                    fixed_str = f"{parts[0]}.{ms}+{tz}"
                elif "-" in parts[1] and parts[1].count("-") > 0:
                    ms, tz = parts[1].rsplit("-", 1)
                    ms = (ms + "000000")[:6]
                    fixed_str = f"{parts[0]}.{ms}-{tz}"
                else:
                    ms = (parts[1] + "000000")[:6]
                    fixed_str = f"{parts[0]}.{ms}"
                return datetime.fromisoformat(fixed_str)
        except:
            pass
    
    return datetime.utcnow()

# --- LINK PARSING UTILITIES ---

def extract_markdown_links(text: str) -> List[Dict[str, str]]:
    """Extract markdown-style links from text"""
    if not text:
        return []
    
    pattern = r'\[([^\]]+)\]\(([^\)]+)\)'
    links = []
    
    for match in re.finditer(pattern, text):
        link_text = match.group(1).strip()
        link_url = match.group(2).strip()
        
        if link_url.startswith('http'):
            links.append({'text': link_text, 'url': link_url})
    
    return links


def categorize_links(links: List[Dict[str, str]]) -> Dict[str, List[Dict[str, str]]]:
    """Categorize links into eBay, FBA, Buy, Other"""
    categories = {'ebay': [], 'fba': [], 'buy': [], 'other': []}
    
    ebay_keywords = ['sold', 'active', 'google', 'ebay']
    fba_keywords = ['keepa', 'amazon', 'selleramp', 'fba', 'camel']
    buy_keywords = ['buy', 'shop', 'purchase', 'checkout', 'cart']
    
    for link in links:
        text_lower = link['text'].lower()
        url_lower = link['url'].lower()
        
        if any(kw in text_lower or kw in url_lower for kw in ebay_keywords):
            categories['ebay'].append(link)
        elif any(kw in text_lower or kw in url_lower for kw in fba_keywords):
            categories['fba'].append(link)
        elif any(kw in text_lower or kw in url_lower for kw in buy_keywords):
            categories['buy'].append(link)
        else:
            categories['other'].append(link)
    
    return categories


# --- IMPROVED IMAGE HANDLING ---

def get_image_dimensions_from_url(url: str) -> Optional[Tuple[int, int]]:
    """
    Extract dimensions from Discord proxy URL parameters or estimate from URL patterns.
    Returns (width, height) or None if cannot determine.
    """
    if not url:
        return None
    
    try:
        # Discord proxy URLs often have width=X&height=Y
        w_match = re.search(r'[?&]width=(\d+)', url)
        h_match = re.search(r'[?&]height=(\d+)', url)
        
        if w_match and h_match:
            return (int(w_match.group(1)), int(h_match.group(1)))
        
        # Amazon URLs with size indicators like _SL160_
        size_match = re.search(r'\._[A-Z]*(\d+)_\.', url)
        if size_match:
            size = int(size_match.group(1))
            return (size, size)
        
        # eBay URLs with s-lXXX pattern
        ebay_match = re.search(r's-l(\d+)\.', url)
        if ebay_match:
            size = int(ebay_match.group(1))
            return (size, size)
            
    except Exception as e:
        logger.debug(f"Could not extract dimensions from URL: {e}")
    
    return None


def is_high_quality_image(url: str, min_dimension: int = 160) -> bool:
    """
    Determine if an image URL is likely high quality without downloading it.
    """
    if not url:
        return False
    
    # Trusted high-res domains - ALWAYS consider these high quality
    trusted_domains = [
        'media-amazon.com',
        'images-amazon.com', 
        'ssl-images-amazon.com',
        'm.media-amazon.com',
        'ebayimg.com'
    ]
    
    url_lower = url.lower()
    
    # Check if it's from a trusted domain
    is_trusted = any(domain in url_lower for domain in trusted_domains)
    
    # Get dimensions if available
    dimensions = get_image_dimensions_from_url(url)
    
    if dimensions:
        width, height = dimensions
        # Image is high quality if either dimension is large enough
        is_large = width >= min_dimension or height >= min_dimension
        
        if is_trusted:
            # For trusted domains, be lenient - even 300px can be good quality
            return width >= 300 or height >= 300
        else:
            return is_large
    
    # If we can't determine dimensions but it's from a trusted domain, assume it's good
    if is_trusted:
        # But still reject obvious thumbnails
        if any(pattern in url_lower for pattern in ['_sl160_', '_ac_uy218_', 's-l300', 'thumb', 'icon']):
            return False
        return True
    
    # For non-Discord proxy URLs from unknown sources, we can't determine without downloading
    # Conservative: return False to trigger scraping
    if 'discordapp.net' not in url_lower:
        return False
    
    return False


def optimize_image_url(url: str) -> str:
    """
    Optimize image URLs to force maximum resolution.
    CRITICAL: Removes size restrictions from Amazon, eBay, and Discord proxy URLs.
    """
    if not url:
        return url
    
    try:
        original_url = url
        
        # 1. Decode Discord Proxy URLs
        if "images-ext-" in url and "discordapp.net" in url:
            if "/https/" in url:
                url = "https://" + url.split("/https/", 1)[1]
            elif "/http/" in url:
                url = "http://" + url.split("/http/", 1)[1]
        
        # 2. Amazon Image Optimization (CRITICAL)
        if any(domain in url for domain in ['media-amazon.com', 'images-amazon.com', 'ssl-images-amazon.com']):
            # Remove ALL size indicators: ._SL160_, ._AC_UY218_, etc.
            url = re.sub(r'\._[A-Z_]+[0-9]+_\.', '.', url)
            
            # Remove query parameters that limit quality
            if "?" in url:
                base_url = url.split("?")[0]
                # Keep only the base URL, no query params
                url = base_url
        
        # 3. eBay Image Optimization
        if "ebayimg.com" in url:
            # Force maximum resolution: s-l300 -> s-l1600
            if re.search(r's-l\d+\.', url):
                url = re.sub(r's-l\d+\.', 's-l1600.', url)
            
            # Remove quality-limiting query params
            if "?" in url:
                url = url.split("?")[0]
        
        # 4. Remove Discord proxy size limits
        if "discordapp.net" in url or "discord.com" in url:
            # Remove width/height parameters
            url = re.sub(r'[?&](width|height)=\d+', '', url)
            # Clean up dangling ? or &
            url = re.sub(r'\?&', '?', url)
            url = re.sub(r'[?&]$', '', url)
        
        if url != original_url:
            logger.debug(f"   ‚ú® Optimized URL: {original_url[:80]} -> {url[:80]}")
        
        return url
                
    except Exception as e:
        logger.error(f"Image optimization error: {e}")
        return url


def download_image_high_quality(image_url: str, max_size_mb: int = 10) -> Optional[bytes]:
    """
    Download image preserving maximum quality.
    Returns raw bytes without compression.
    """
    if not image_url or not image_url.startswith('http'):
        return None
    
    # Optimize URL first to get best quality
    image_url = optimize_image_url(image_url)
    
    try:
        # Realistic headers to avoid 403 errors
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://www.google.com/',
            'Sec-Fetch-Dest': 'image',
            'Sec-Fetch-Mode': 'no-cors',
            'Sec-Fetch-Site': 'cross-site',
            'Cache-Control': 'no-cache'
        }
        
        # Add domain-specific headers
        if 'amazon' in image_url:
            headers['Referer'] = 'https://www.amazon.com/'
        elif 'ebay' in image_url:
            headers['Referer'] = 'https://www.ebay.com/'
        
        response = requests.get(image_url, headers=headers, timeout=15, stream=True)
        response.raise_for_status()
        
        # Check content type
        content_type = response.headers.get('Content-Type', '')
        if not content_type.startswith('image/'):
            logger.warning(f"   ‚ö†Ô∏è Invalid content type: {content_type}")
            return None
        
        # Download with size limit
        max_size_bytes = max_size_mb * 1024 * 1024
        downloaded = b''
        
        for chunk in response.iter_content(chunk_size=8192):
            if chunk:
                downloaded += chunk
                if len(downloaded) > max_size_bytes:
                    logger.warning(f"   ‚ö†Ô∏è Image too large (>{max_size_mb}MB)")
                    return None
        
        # Verify it's a valid image by trying to open it
        try:
            img = Image.open(BytesIO(downloaded))
            width, height = img.size
            logger.info(f"   ‚úÖ Downloaded image: {width}x{height} ({len(downloaded)//1024}KB)")
            
            # CRITICAL: Return original bytes, DO NOT re-encode
            return downloaded
            
        except Exception as img_err:
            logger.warning(f"   ‚ö†Ô∏è Invalid image data: {img_err}")
            return None
        
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 403:
            logger.warning(f"   üö´ 403 Forbidden: {image_url[:80]}")
        elif e.response.status_code == 404:
            logger.warning(f"   ‚ùå 404 Not Found: {image_url[:80]}")
        else:
            logger.warning(f"   ‚ö†Ô∏è HTTP {e.response.status_code}: {image_url[:80]}")
        return None
        
    except requests.exceptions.Timeout:
        logger.warning(f"   ‚è±Ô∏è Timeout downloading: {image_url[:80]}")
        return None
        
    except Exception as e:
        logger.warning(f"   ‚ö†Ô∏è Download error: {str(e)[:80]}")
        return None


def fetch_product_images(url: str, max_images: int = 3) -> List[str]:
    """
    Scrape high-res product images from a URL with improved anti-blocking.
    """
    # Skip known non-product pages
    skip_scrape = any(x in url.lower() for x in [
        'keepa.com', 'ebay.com/sch', 'camelcamelcamel',
        'login', 'cart', 'checkout', 'account', 'signin'
    ])
    
    if skip_scrape:
        logger.debug(f"   ‚è≠Ô∏è Skipping scrape of non-product page")
        return []
    
    # Enhanced User-Agent rotation
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    ]
    
    headers = {
        'User-Agent': random.choice(user_agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9',
        'Accept-Encoding': 'gzip, deflate, br',
        'Referer': 'https://www.google.com/',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Sec-Fetch-User': '?1',
        'Cache-Control': 'max-age=0'
    }
    
    images = []
    
    try:
        if not url or not url.startswith('http'):
            return []
        
        response = requests.get(url, headers=headers, timeout=12)
        
        if response.status_code == 403:
            logger.warning(f"   üö´ 403 Forbidden - site blocking scraper")
            return []
        
        if response.status_code != 200:
            logger.warning(f"   ‚ö†Ô∏è Scrape failed: HTTP {response.status_code}")
            return []
        
        soup = BeautifulSoup(response.text, 'html.parser')
        skip_keywords = ['logo', 'icon', 'banner', 'button', 'sprite', 'loading', 'placeholder', 'blank', 'ajax']
        
        # Priority 0: Meta Tags
        for meta in soup.find_all('meta'):
            prop = meta.get('property', '')
            name = meta.get('name', '')
            if prop in ['og:image', 'twitter:image'] or name in ['og:image', 'twitter:image']:
                meta_url = meta.get('content')
                if meta_url:
                    if meta_url.startswith('//'):
                        meta_url = 'https:' + meta_url
                    elif not meta_url.startswith('http'):
                        from urllib.parse import urljoin
                        meta_url = urljoin(url, meta_url)
                    
                    if meta_url.startswith('http') and not any(k in meta_url.lower() for k in skip_keywords):
                        images.append({
                            'url': meta_url,
                            'alt': 'Meta Tag Image',
                            'priority': 1000
                        })
        
        # Priority 1: JSON-LD Structured Data
        try:
            scripts = soup.find_all('script', type='application/ld+json')
            for script in scripts:
                if script.string:
                    try:
                        data = json.loads(script.string)
                        items = []
                        if isinstance(data, list):
                            items = data
                        elif isinstance(data, dict):
                            if '@graph' in data:
                                items.extend(data['@graph'])
                            items.append(data)
                        
                        for item in items:
                            if item.get('@type') in ['Product', 'ItemPage', 'IndividualProduct']:
                                img_data = item.get('image')
                                found_imgs = []
                                
                                if isinstance(img_data, str):
                                    found_imgs.append(img_data)
                                elif isinstance(img_data, list):
                                    for i in img_data:
                                        if isinstance(i, str):
                                            found_imgs.append(i)
                                        elif isinstance(i, dict) and 'url' in i:
                                            found_imgs.append(i['url'])
                                elif isinstance(img_data, dict) and 'url' in img_data:
                                    found_imgs.append(img_data['url'])
                                
                                for img_url in found_imgs:
                                    if img_url:
                                        if img_url.startswith('//'):
                                            img_url = 'https:' + img_url
                                        elif not img_url.startswith('http'):
                                            from urllib.parse import urljoin
                                            img_url = urljoin(url, img_url)
                                        
                                        if img_url.startswith('http') and not any(k in img_url.lower() for k in skip_keywords):
                                            images.append({
                                                'url': img_url,
                                                'alt': 'JSON-LD Image',
                                                'priority': 950
                                            })
                    except:
                        continue
        except Exception as json_err:
            logger.debug(f"   JSON-LD parse error: {json_err}")
        
        # Priority 2: Img Tags
        for img in soup.find_all('img'):
            img_url = img.get('src') or img.get('data-src') or img.get('data-lazy-src')
            if not img_url:
                continue
            
            if img_url.startswith('//'):
                img_url = 'https:' + img_url
            elif not img_url.startswith('http'):
                from urllib.parse import urljoin
                img_url = urljoin(url, img_url)
            
            if not img_url.startswith('http') or img_url.startswith('data:'):
                continue
            
            # Size check
            if 'width' in img.attrs and 'height' in img.attrs:
                try:
                    w = int(str(img['width']).replace('px', ''))
                    h = int(str(img['height']).replace('px', ''))
                    if w < 100 or h < 100:
                        continue
                except:
                    pass
            
            img_url_lower = img_url.lower()
            if any(skip in img_url_lower for skip in skip_keywords):
                continue
            
            score = 0
            alt_text = img.get('alt', '').lower()
            if 'product' in img_url_lower or 'product' in alt_text:
                score += 50
            if 'main' in img_url_lower:
                score += 20
            if 'gallery' in img_url_lower:
                score += 10
            if 'cdn.shopify.com' in img_url_lower:
                score += 30
            
            if score > 0:
                images.append({'url': img_url, 'priority': score})
        
        # Sort and deduplicate
        images.sort(key=lambda x: x['priority'], reverse=True)
        seen = set()
        final_images = []
        
        for img in images:
            if img['url'] not in seen and img['url']:
                seen.add(img['url'])
                final_images.append(img['url'])
        
        if final_images:
            logger.info(f"   üì∏ Scraped {len(final_images)} image(s)")
        
        return final_images[:max_images]
    
    except requests.exceptions.Timeout:
        logger.warning(f"   ‚è±Ô∏è Scrape timeout")
        return []
    except requests.exceptions.RequestException as e:
        logger.warning(f"   ‚ö†Ô∏è Scrape error: {str(e)[:80]}")
        return []
    except Exception as e:
        logger.error(f"   ‚ùå Scrape error: {str(e)[:80]}")
        return []


def add_emoji_to_link_text(text: str) -> str:
    """Add emoji prefix to link text for better visual appeal"""
    emojis = {
        'sold': 'üí∞', 'active': '‚ö°', 'google': 'üîç', 'ebay': 'üõí',
        'keepa': 'üìà', 'amazon': 'üîé', 'selleramp': 'üíé', 'camel': 'üê´',
        'buy': 'üõí', 'shop': 'ü™Ä', 'cart': 'üõí', 'checkout': '‚úÖ',
    }
    
    text_lower = text.lower()
    for keyword, emoji in emojis.items():
        if keyword in text_lower:
            return f"{emoji} {text}"
    
    return f"üîó {text}"


def parse_tag_line(tag: str) -> Dict[str, Optional[str]]:
    """Parse Discord tag line above embeds."""
    if not tag:
        return {}
    
    parts = [p.strip() for p in tag.split('|')]
    result = {
        'ping': None, 'region': None, 'product_code': None,
        'brand': None, 'action': None, 'price': None, 'raw': tag
    }
    
    for part in parts:
        if part.startswith('@'):
            result['ping'] = part
        elif part.startswith('[') and part.endswith(']'):
            result['region'] = part.strip('[]')
        elif any(kw in part.lower() for kw in ['restocked', 'in stock', 'available']):
            result['action'] = part
            price_match = re.search(r'[¬£$‚Ç¨]\s*[\d,]+\.?\d*', part)
            if price_match:
                result['price'] = price_match.group(0)
        else:
            if result['product_code'] is None:
                result['product_code'] = part
            elif result['brand'] is None:
                result['brand'] = part
    
    return result


# --- SUBSCRIPTION MANAGER ---

class SubscriptionManager:
    def __init__(self):
        self.users: Dict[str, Dict] = {} 
        self.codes: Dict[str, int] = {}
        self.potential_users: Dict[str, Dict] = {}
        self.lock = threading.Lock()
        self.remote_users_path = f"discord_josh/{USERS_FILE}"
        self.remote_codes_path = f"discord_josh/{CODES_FILE}"
        self.remote_potential_path = f"discord_josh/{POTENTIAL_USERS_FILE}"
        self.local_users_path = f"data/{USERS_FILE}"
        self.local_codes_path = f"data/{CODES_FILE}"
        self.local_potential_path = f"data/{POTENTIAL_USERS_FILE}"
        os.makedirs("data", exist_ok=True)
        self._load_state()

    def _load_state(self):
        """Load state from Supabase with local fallback"""
        users_loaded = False
        try:
            data = supabase_utils.download_file(self.local_users_path, self.remote_users_path, SUPABASE_BUCKET)
            if data: 
                self.users = json.loads(data)
                users_loaded = True
                logger.info(f"‚úÖ Loaded {len(self.users)} users from Supabase")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to download users from Supabase: {e}")
            
        if not users_loaded and os.path.exists(self.local_users_path):
            try:
                with open(self.local_users_path, 'r') as f:
                    self.users = json.load(f)
                logger.info(f"üìÇ Loaded {len(self.users)} users from local fallback")
            except Exception as e:
                logger.error(f"‚ùå Failed to load local users fallback: {e}")

        codes_loaded = False
        try:
            data = supabase_utils.download_file(self.local_codes_path, self.remote_codes_path, SUPABASE_BUCKET)
            if data:
                self.codes = json.loads(data)
                codes_loaded = True
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to download codes from Supabase: {e}")

        if not codes_loaded and os.path.exists(self.local_codes_path):
            try:
                with open(self.local_codes_path, 'r') as f:
                    self.codes = json.load(f)
            except: pass

        potential_loaded = False
        try:
            data = supabase_utils.download_file(self.local_potential_path, self.remote_potential_path, SUPABASE_BUCKET)
            if data:
                self.potential_users = json.loads(data)
                potential_loaded = True
        except: pass

        if not potential_loaded and os.path.exists(self.local_potential_path):
            try:
                with open(self.local_potential_path, 'r') as f:
                    self.potential_users = json.load(f)
            except: pass

    def _sync_state(self):
        try:
            with open(self.local_users_path, 'w') as f: json.dump(self.users, f)
            supabase_utils.upload_file(self.local_users_path, SUPABASE_BUCKET, self.remote_users_path, debug=False)
            with open(self.local_codes_path, 'w') as f: json.dump(self.codes, f)
            supabase_utils.upload_file(self.local_codes_path, SUPABASE_BUCKET, self.remote_codes_path, debug=False)
            with open(self.local_potential_path, 'w') as f: json.dump(self.potential_users, f)
            supabase_utils.upload_file(self.local_potential_path, SUPABASE_BUCKET, self.remote_potential_path, debug=False)
        except Exception as e:
            logger.error(f"Sync error: {e}")

    def generate_code(self, days: int) -> str:
        import secrets
        code = secrets.token_hex(4).upper()
        with self.lock:
            self.codes[code] = days
            self._sync_state()
        return code

    def redeem_code(self, user_id: str, username: str, code: str) -> bool:
        with self.lock:
            if code not in self.codes: return False
            days = self.codes.pop(code)
            current_expiry = datetime.utcnow()
            if str(user_id) in self.users:
                try:
                    old_expiry = parse_iso_datetime(self.users[str(user_id)]["expiry"])
                    if old_expiry > datetime.utcnow(): current_expiry = old_expiry
                except: pass
            
            new_expiry = current_expiry + timedelta(days=days)
            self.users[str(user_id)] = {
                "expiry": new_expiry.isoformat(), 
                "username": username or "Unknown",
                "alerts_paused": False,
                "joined_at": self.users.get(str(user_id), {}).get("joined_at", datetime.utcnow().isoformat())
            }
            if str(user_id) in self.potential_users:
                self.potential_users.pop(str(user_id))
            self._sync_state()
            return True

    def get_active_users(self) -> List[str]:
        active = []
        now = datetime.utcnow()
        with self.lock:
            for uid, data in self.users.items():
                try:
                    if parse_iso_datetime(data["expiry"]) > now:
                        if not data.get("alerts_paused", False):
                            active.append(uid)
                except: pass
        return active
    
    def get_expiry(self, user_id: str):
        return self.users.get(str(user_id), {}).get("expiry")
    
    def is_active(self, user_id: str) -> bool:
        expiry = self.get_expiry(str(user_id))
        if not expiry: return False
        try:
            return parse_iso_datetime(expiry) > datetime.utcnow()
        except:
            return False
    
    def toggle_pause(self, user_id: str) -> bool:
        """Toggle pause status, returns new paused state"""
        with self.lock:
            if str(user_id) not in self.users:
                return False
            current = self.users[str(user_id)].get("alerts_paused", False)
            self.users[str(user_id)]["alerts_paused"] = not current
            self._sync_state()
            return not current
    
    def get_user_stats(self, user_id: str) -> Dict:
        """Get user statistics"""
        user_data = self.users.get(str(user_id), {})
        if not user_data:
            return None
        
        expiry = parse_iso_datetime(user_data["expiry"])
        joined = parse_iso_datetime(user_data.get("joined_at", datetime.utcnow().isoformat()))
        now = datetime.utcnow()
        
        return {
            "username": user_data.get("username", "Unknown"),
            "days_remaining": (expiry - now).days if expiry > now else 0,
            "days_active": (now - joined).days,
            "is_paused": user_data.get("alerts_paused", False),
            "expiry_date": expiry.strftime("%Y-%m-%d %H:%M UTC"),
            "is_admin": user_data.get("is_admin", False)
        }

    def add_admin(self, user_id: str) -> bool:
        """Set a user as admin"""
        with self.lock:
            if str(user_id) not in self.users:
                # Add a placeholder user if they don't exist yet? 
                # Or just error out. Let's assume they must be in the system.
                return False
            self.users[str(user_id)]["is_admin"] = True
            self._sync_state()
            return True

    def remove_admin(self, user_id: str) -> bool:
        """Remove admin status from a user"""
        with self.lock:
            if str(user_id) in self.users:
                self.users[str(user_id)]["is_admin"] = False
                self._sync_state()
                return True
            return False

    def is_bot_admin(self, user_id: str) -> bool:
        """Check if a user is a secondary admin"""
        return self.users.get(str(user_id), {}).get("is_admin", False)

    def get_all_admins(self) -> List[str]:
        """Get list of all admin IDs (Superadmin + Secondary)"""
        admins = [str(ADMIN_USER_ID)]
        with self.lock:
            for uid, data in self.users.items():
                if data.get("is_admin"):
                    if uid not in admins:
                        admins.append(uid)
        return admins

    def get_expired_users_needing_reminder(self) -> List[str]:
        """Get users with expired subscriptions who haven't been reminded in 14 days"""
        needing_reminder = []
        now = datetime.utcnow()
        with self.lock:
            for uid, data in self.users.items():
                try:
                    expiry = parse_iso_datetime(data["expiry"])
                    if expiry < now:
                        last_reminder_str = data.get("last_expiry_reminder")
                        should_remind = False
                        
                        if not last_reminder_str:
                            should_remind = True
                        else:
                            last_reminder = parse_iso_datetime(last_reminder_str)
                            if (now - last_reminder).days >= 14:
                                should_remind = True
                        
                        if should_remind:
                            needing_reminder.append(uid)
                except Exception as e:
                    logger.error(f"Error checking reminder for {uid}: {e}")
        return needing_reminder

    def update_reminder_timestamp(self, user_id: str):
        """Update last_expiry_reminder timestamp only and sync to Supabase"""
        with self.lock:
            uid = str(user_id)
            if uid in self.users:
                self.users[uid]["last_expiry_reminder"] = datetime.utcnow().isoformat()
                self._sync_state()

    def track_potential_user(self, user_id: str, username: str):
        """Track user who ran /start but isn't subscribed"""
        with self.lock:
            uid = str(user_id)
            # Only add if not an active user and not already in potential list
            if uid not in self.users and uid not in self.potential_users:
                self.potential_users[uid] = {
                    "username": username or "Unknown",
                    "first_seen": datetime.utcnow().isoformat(),
                    "last_reminder": None
                }
                self._sync_state()

    def get_potential_users_needing_reminder(self) -> List[str]:
        """Get potential users who haven't been reminded in 14 days"""
        needing_reminder = []
        now = datetime.utcnow()
        with self.lock:
            for uid, data in self.potential_users.items():
                last_reminder_str = data.get("last_reminder")
                if not last_reminder_str:
                    # First reminder after 14 days of joining
                    first_seen = parse_iso_datetime(data["first_seen"])
                    if (now - first_seen).days >= 14:
                        needing_reminder.append(uid)
                else:
                    last_reminder = parse_iso_datetime(last_reminder_str)
                    if (now - last_reminder).days >= 14:
                        needing_reminder.append(uid)
        return needing_reminder

    def update_potential_reminder_timestamp(self, user_id: str):
        """Update last_reminder timestamp for potential user"""
        with self.lock:
            uid = str(user_id)
            if uid in self.potential_users:
                self.potential_users[uid]["last_reminder"] = datetime.utcnow().isoformat()
                self._sync_state()


# --- MESSAGE POLLER ---

class MessagePoller:
    def __init__(self):
        self.last_scraped_at = None
        self.sent_ids = set()
        self.recent_signatures = []  # Last 3 sent content signatures
        self.supabase_url, self.supabase_key = supabase_utils.get_supabase_config()
        self.cursor_file = "bot_cursor.json"
        self.local_path = f"data/{self.cursor_file}"
        self.remote_path = f"discord_josh/{self.cursor_file}"
        self.time_based_signatures = {}  # {sig_hash: timestamp_iso}
        self._init_cursor()

    def _init_cursor(self):
        try:
            data = supabase_utils.download_file(self.local_path, self.remote_path, SUPABASE_BUCKET)
            if data: 
                loaded = json.loads(data)
                self.last_scraped_at = loaded.get("last_scraped_at")
                # Support migration from sent_hashes to sent_ids
                self.sent_ids = set(loaded.get("sent_ids", loaded.get("sent_hashes", [])))
                # Load persistent signatures
                self.recent_signatures = loaded.get("recent_signatures", [])
                # Load time-based signatures
                self.time_based_signatures = loaded.get("time_based_signatures", {})
            if not self.last_scraped_at: 
                self.last_scraped_at = (datetime.utcnow() - timedelta(hours=24)).isoformat()
        except:
            self.last_scraped_at = (datetime.utcnow() - timedelta(hours=24)).isoformat()

    def _save_cursor(self):
        try:
            with open(self.local_path, 'w') as f: 
                json.dump({
                    "last_scraped_at": self.last_scraped_at,
                    "sent_ids": list(self.sent_ids)[-5000:],  # Increased to 5000 IDs
                    "recent_signatures": self.recent_signatures[-20:], # Increased to last 20
                    "time_based_signatures": self.time_based_signatures
                }, f)
            supabase_utils.upload_file(self.local_path, SUPABASE_BUCKET, self.remote_path, debug=False)
        except: pass

    def _get_content_signature(self, msg: Dict) -> str:
        """Generate a signature for content-based deduplication (Retailer + Title + Price)"""
        try:
            raw = msg.get("raw_data", {})
            embed = raw.get("embed") or {}
            content = msg.get("content", "")
            
            retailer = ""
            title = ""
            price = ""
            
            # 1. Try Embed extraction
            if embed.get("author"):
                retailer = embed["author"].get("name", "")
            
            title = embed.get("title", "")
            
            # Extract price from embed fields
            for field in embed.get("fields", []):
                name = (field.get("name") or "").lower()
                if "price" in name:
                    price = field.get("value", "")
                    break
            
            # 2. FALLBACK: Parse plain text content if embed data missing
            if not retailer or not title or not price:
                # Pattern: "@Mention | Product Name | Retailer | Just restocked for ¬£XX.XX"
                if content and "|" in content:
                    parts = [p.strip() for p in content.split("|")]
                    if len(parts) >= 3:
                        # parts[0] is usually @Mention, but sometimes it's the product
                        # We'll use the logic that if we still need title/retailer, we try to grab them
                        if not title and len(parts) > 1:
                            title = parts[1]
                        if not retailer and len(parts) > 2:
                            retailer = parts[2]
                        if not price:
                            # Extract price from any part that matches a currency pattern
                            # Search the whole content for the most likely price
                            price_match = re.search(r'[¬£$‚Ç¨]\s*[\d,]+\.?\d*', content)
                            if price_match:
                                price = price_match.group(0)
                
            # Final fallback for Argos or other specific keywords if still empty
            if not retailer and "Argos" in content:
                retailer = "Argos Instore"
            
            # Create raw signature and hash it
            raw_sig = f"{retailer}|{title}|{price}".lower().strip()
            
            # If everything is still empty, use content hash or ID
            if raw_sig == "||":
                return hashlib.md5(content.encode()).hexdigest() if content else str(msg.get("id"))
                
            import hashlib
            return hashlib.md5(raw_sig.encode()).hexdigest()
        except Exception as e:
            logger.error(f"Error generating signature: {e}")
            return str(msg.get("id"))

    def poll_new_messages(self):
        try:
            if not self.last_scraped_at: 
                self.last_scraped_at = (datetime.utcnow() - timedelta(minutes=45)).isoformat()
            
            headers = {"apikey": self.supabase_key, "Authorization": f"Bearer {self.supabase_key}"}
            url = f"{self.supabase_url}/rest/v1/discord_messages"
            params = {"scraped_at": f"gt.{self.last_scraped_at}", "order": "scraped_at.asc"}
            
            res = requests.get(url, headers=headers, params=params, timeout=45)
            if res.status_code != 200: return []
            
            messages = res.json()
            now_iso = datetime.utcnow().isoformat()
            now_dt = datetime.utcnow()
            
            # Prune time-based signatures older than 10 minutes
            pruned_sigs = {}
            for s, ts in self.time_based_signatures.items():
                if (now_dt - parse_iso_datetime(ts)) < timedelta(minutes=10):
                    pruned_sigs[s] = ts
            self.time_based_signatures = pruned_sigs

            new_messages = []
            for msg in messages:
                msg_id = msg.get("id")
                # LAYER 1: Discord ID Check (All-time tracking)
                if not msg_id or str(msg_id) in self.sent_ids:
                    continue
                
                sig = self._get_content_signature(msg)
                
                # LAYER 2: Content Signature (Sliding Window - last 20)
                if sig in self.recent_signatures:
                    logger.info(f"‚è≠Ô∏è LAYER 2 BLOCK: Duplicate content window: {msg_id} (Sig: {sig})")
                    self.sent_ids.add(str(msg_id))
                    continue
                
                # LAYER 3: Time-Based Deduplication (10-minute window)
                if sig in self.time_based_signatures:
                    logger.info(f"‚è≠Ô∏è LAYER 3 BLOCK: Duplicate content within 10m: {msg_id} (Sig: {sig})")
                    self.sent_ids.add(str(msg_id))
                    continue

                # MESSAGE ACCEPTED - Add to tracking IMMEDIATELY
                new_messages.append(msg)
                self.sent_ids.add(str(msg_id))
                self.recent_signatures.append(sig)
                self.recent_signatures = self.recent_signatures[-20:] # Keep last 20
                self.time_based_signatures[sig] = now_iso
            
            if new_messages:
                # Save cursor immediately after polling if new messages found
                self._save_cursor()
                
            return new_messages
        except Exception as e:
            logger.error(f"Poll error: {e}")
            return []
    
    def update_cursor(self, scraped_at: str, msg_data: Optional[Dict] = None):
        """Update cursor to given scraped_at timestamp"""
        self.last_scraped_at = scraped_at
        self._save_cursor()


sm = SubscriptionManager()
poller = MessagePoller()

# --- ADMIN PERMISSION HELPERS ---

def is_superadmin(user_id: str) -> bool:
    """Check if user is the main admin from .env"""
    return str(user_id) == str(ADMIN_USER_ID)

def is_admin(user_id: str) -> bool:
    """Check if user is either superadmin or secondary admin"""
    uid_str = str(user_id)
    return is_superadmin(uid_str) or sm.is_bot_admin(uid_str)

async def notify_admins(context: ContextTypes.DEFAULT_TYPE, text: str):
    """Notify all admins of an event/error"""
    admin_ids = sm.get_all_admins()
    for aid in admin_ids:
        try:
            await context.bot.send_message(chat_id=aid, text=text, parse_mode=ParseMode.HTML)
        except Exception as e:
            logger.error(f"Failed to notify admin {aid}: {e}")


# --- PROFESSIONAL MESSAGE FORMATTING ---

# Phrases to remove from messages
PHRASES_TO_REMOVE = [
    "CCN 2.0 | Profitable Pinger",
    " Monitors v2.0.0 | CCN x Zephyr Monitors #ad",
    " Monitors v2.0.0 | CCN x Zephyr Monitors",
    "CCN 2.0 | Profitable Pinger",
    "@Unfiltered",
    "CCN",
    "@Product Flips"
]

# Regex patterns to remove (for dynamic content like timestamps)
REGEX_PATTERNS_TO_REMOVE = [
    r'Monitors\s+v[\d.]+\s*\|\s*CCN\s+x\s+Zephyr\s+Monitors\s*\[\d{2}:\d{2}:\d{2}\]',
    r'\s*\|\s*CCN\s+x\s+Zephyr\s+Monitors\s+\[[^\]]+\].*',
    r'Today\s+at\s+\d{1,2}:\d{2}\s*(?:AM|PM)',
]

def clean_text(text: str) -> str:
    """Remove unwanted phrases from text"""
    if not text:
        return text
    
    # Remove literal phrases
    for phrase in PHRASES_TO_REMOVE:
        text = re.sub(re.escape(phrase), "", text, flags=re.IGNORECASE)
    
    # Remove patterns with regex
    for pattern in REGEX_PATTERNS_TO_REMOVE:
        text = re.sub(pattern, "", text, flags=re.IGNORECASE)
    
    # Clean up extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def format_price_value(value: str) -> str:
    """
    Format price value for Telegram with smart discount detection.
    
    Detects patterns like:
    - '2.95 (-32%) 1.99' -> '<s>¬£2.95</s> (-32%) <b>¬£1.99</b>'
    - '¬£2.95 (-32%) ¬£1.99' -> '<s>¬£2.95</s> (-32%) <b>¬£1.99</b>'
    - '~~2.95~~ 1.99' -> '<s>¬£2.95</s> <b>¬£1.99</b>'
    - Simple prices like '5.99' -> '¬£5.99'
    """
    if not value: 
        return value
    
    # Strip leading/trailing whitespace
    value = value.strip()
    
    # Helper to ensure currency symbol
    def ensure_currency(price_str):
        price_str = price_str.strip()
        if price_str and not any(c in price_str for c in ['¬£', '$', '‚Ç¨']):
            return f"¬£{price_str}"
        return price_str
    
    # Pattern 1: Detect discount format "ORIGINAL_PRICE (PERCENT%) DISCOUNTED_PRICE"
    # Matches: "2.95 (-32%) 1.99", "¬£2.95 (-32%) ¬£1.99", "0.95 (-47%) 0.5"
    discount_pattern = r'([¬£$‚Ç¨]?\d+(?:[.,]\d{1,2})?)[\s]*\((-?\d+%?)\)[\s]*([¬£$‚Ç¨]?\d+(?:[.,]\d{1,2})?)'
    
    match = re.search(discount_pattern, value)
    if match:
        original_price = ensure_currency(match.group(1))
        discount_percent = match.group(2)
        # Ensure percent has % if it doesn't
        if '%' not in discount_percent:
            discount_percent = f"{discount_percent}%"
        discounted_price = ensure_currency(match.group(3))
        
        # Format: strikethrough original, bold discounted
        return f"<s>{original_price}</s> ({discount_percent}) <b>{discounted_price}</b>"
    
    # Pattern 2: Discord markdown strikethrough ~~text~~
    if '~~' in value:
        # Convert ~~text~~ to <s>text</s>
        value = re.sub(r'~~([^~]+)~~', r'<s>\1</s>', value)
        
        # Find any remaining prices and ensure they have currency + bold the last one
        prices = re.findall(r'[¬£$‚Ç¨]?\d+(?:[.,]\d{1,2})?', value)
        if prices:
            last_price = prices[-1]
            # Bold the discounted price (last price not in strikethrough)
            if f"<s>{last_price}</s>" not in value and f"<s>¬£{last_price}</s>" not in value:
                value = value.replace(last_price, f"<b>{ensure_currency(last_price)}</b>", 1)
    
    # Pattern 3: Simple price without discount - just ensure currency symbol
    # Only add currency if it's a simple standalone price
    simple_price_pattern = r'^[¬£$‚Ç¨]?\d+(?:[.,]\d{1,2})?$'
    if re.match(simple_price_pattern, value.strip()):
        return ensure_currency(value.strip())
    
    # Fallback: ensure all standalone numeric prices have currency symbols
    # But don't match numbers that are part of a decimal (like "99" in "2.99")
    # And don't add ¬£ to numbers that are followed by currency codes like USD, EUR
    def add_currency_to_prices(text):
        def replacer(m):
            price = m.group(0)
            # Check what comes after this match in the original text
            end_pos = m.end()
            suffix = text[end_pos:end_pos+10].strip().upper()
            
            # Don't add ¬£ if followed by currency code (it already has a denomination)
            if suffix.startswith(('USD', 'EUR', 'GBP', 'CAD', 'AUD')):
                return price  # Leave as-is, it has a currency label
            
            if not any(c in price for c in ['¬£', '$', '‚Ç¨']):
                return f"¬£{price}"
            return price
        # Match complete prices only - must not be preceded by decimal point, currency, or digit
        # and must not be followed by % or digit (unless it's the decimal portion)
        # Added support for commas as thousands separators: (?:,\d{3})*
        return re.sub(r'(?<![¬£$‚Ç¨\d.])\d+(?:,\d{3})*(?:\.\d{1,2})?(?![%\d])', replacer, text)
    
    return add_currency_to_prices(value)


# --- CHANNEL SPECIFIC FORMATTERS ---

CHANNEL_COLLECTORS = "1367813504786108526"
CHANNEL_ARGOS = "855164313006505994"
CHANNEL_RESTOCKS = "864504557903937587"

def _format_collectors_amazon(msg_data: Dict, embed: Dict) -> Tuple[List[str], List[List[InlineKeyboardButton]]]:
    """Formatter for Collectors Edge / Amazon V3 (Channel 136...)"""
    lines = []
    keyboard = []
    
    # 1. Header & Title (Retailer or Author)
    author_name = msg_data.get("raw_data", {}).get("author", {}).get("name")
    if not author_name and embed.get("author"):
        author_name = embed["author"].get("name")
        
    if author_name and "unknown" not in author_name.lower():
        lines.append(f"üè™ <b>{clean_text(author_name)}</b>")
        lines.append("")

    title = clean_text(embed.get("title", "Product Alert"))
    lines.append(f"üì¶ <b>{title}</b>")
    lines.append("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    lines.append("")
    
    # 2. ALL Fields (comprehensive)
    fields = embed.get("fields", [])
    seen_values = set()
    
    for f in fields:
        name = clean_text(f.get("name", ""))
        val = clean_text(f.get("value", ""))
        
        # Skip link/button fields and N/A values
        if not name or not val: continue
        if any(kw in name.lower() for kw in ['link', 'atc', 'qt', 'checkout']): continue
        if "n/a" in val.lower(): continue
        if val in seen_values: continue
        seen_values.add(val)
        
        # Smart icon selection
        name_lower = name.lower()
        if "price" in name_lower: icon = "üí∞"
        elif "stock" in name_lower or "in stock" in name_lower: icon = "‚úÖ"
        elif "type" in name_lower: icon = "üîñ"
        elif "size" in name_lower: icon = "üìè"
        elif "quantity" in name_lower or "qty" in name_lower: icon = "üî¢"
        else: icon = "‚Ä¢"
        
        # Add currency symbol and bold prices
        if "price" in name_lower:
            val = format_price_value(val)
            lines.append(f"{icon} <b>{name}:</b> <b>{val}</b>")
        else:
            lines.append(f"{icon} <b>{name}:</b> {val}")
        
    lines.append("")
    
    return lines, []

def _format_argos(msg_data: Dict, embed: Dict) -> Tuple[List[str], List[List[InlineKeyboardButton]]]:
    """Formatter for Argos Instore (Channel 855...)"""
    lines = []
    
    lines.append("üè™ <b>Argos Instore</b>")
    lines.append("")
    
    title = clean_text(embed.get("title", "Item Restock"))
    lines.append(f"üì¶ <b>{title}</b>")
    lines.append("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    lines.append("")
    
    # Display ALL fields
    fields = embed.get("fields", [])
    seen_values = set()
    
    for f in fields:
        name = clean_text(f.get("name", ""))
        val = clean_text(f.get("value", ""))
        
        if not name or not val: continue
        if any(kw in name.lower() for kw in ['link', 'atc', 'qt', 'checkout', 'offer id']): continue
        if "n/a" in val.lower(): continue
        if val in seen_values: continue
        seen_values.add(val)
        
        name_lower = name.lower()
        if "store" in name_lower or "availability" in name_lower: icon = "üìç"
        elif "price" in name_lower: icon = "üí∞"
        elif "stock" in name_lower: icon = "‚úÖ"
        elif "size" in name_lower: icon = "üìè"
        else: icon = "‚Ä¢"
        
        # Add currency to prices
        if "price" in name_lower and val:
            val = format_price_value(val)
        
        lines.append(f"{icon} <b>{name}:</b> {val}")
    
    return lines, []

def _format_restocks_currys(msg_data: Dict, embed: Dict) -> Tuple[List[str], List[List[InlineKeyboardButton]]]:
    """Formatter for Online Restocks / Currys (Channel 864...)"""
    lines = []
    
    # This channel puts "Product Info" with "Just Restocked At..."
    fields = embed.get("fields", [])
    prod_info = ""
    resell = ""
    price = ""
    
    for f in fields:
        name = clean_text(f.get("name", "")).lower()
        val = clean_text(f.get("value", ""))
        if "product info" in name: prod_info = val
        if "resell" in name and "n/a" not in val.lower(): resell = val
        if "price" in name and "n/a" not in val.lower(): price = val
        
    # Extract site from product info if possible
    site = "Online Restocks"
    if "just restocked at" in prod_info.lower():
        try:
            site = prod_info.lower().split("just restocked at")[-1].strip()
            site = site.replace("**", "").strip()
        except: pass
        
    lines.append(f"‚ö° <b>{site.upper()}</b>")
    lines.append("")
    
    title = clean_text(embed.get("title", "Product"))
    lines.append(f"üì¶ <b>{title}</b>")
    lines.append("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
    lines.append("")
    
    # Display ALL fields with smart prioritization
    seen_values = set()
    for f in fields:
        name = clean_text(f.get("name", ""))
        val = clean_text(f.get("value", ""))
        
        if not name or not val: continue
        if any(kw in name.lower() for kw in ['link', 'atc', 'qt', 'checkout', 'offer id']): continue  
        if "n/a" in val.lower(): continue
        if val in seen_values: continue
        seen_values.add(val)
        
        name_lower = name.lower()
        if "price" in name_lower: icon = "üí∞"
        elif "resell" in name_lower or "profit" in name_lower: icon = "üìà"
        elif "type" in name_lower: icon = "üîñ"
        elif "stock" in name_lower: icon = "‚úÖ"
        elif "product info" in name_lower: icon = "‚ÑπÔ∏è"
        else: icon = "‚Ä¢"
        
        if "price" in name_lower:
            val = format_price_value(val)
            lines.append(f"{icon} <b>{name}:</b> <b>{val}</b>")
        else:
            lines.append(f"{icon} <b>{name}:</b> {val}")
        
    lines.append("")
    
    return lines, []

# Utility functions consolidated at the top of the file


def format_telegram_message(msg_data: Dict) -> Tuple[str, Optional[str], Optional[InlineKeyboardMarkup], bool]:
    """
    Dispatcher for channel-specific formatting with Fallback to Generic.
    """
    raw = msg_data.get("raw_data", {})
    embed = raw.get("embed")
    # author = raw.get("author", {}) # Unused
    plain_content = msg_data.get("content", "")
    channel_id = str(msg_data.get("channel_id", ""))
    
    # Parse tag info just in case
    tag_info = parse_tag_line(plain_content) if plain_content else {}
    
    lines = []
    custom_buttons = []
    
    # === DISPATCHER ===
    if embed:
        if channel_id == CHANNEL_COLLECTORS:
            l, b = _format_collectors_amazon(msg_data, embed)
            lines.extend(l)
            custom_buttons.extend(b)
        elif channel_id == CHANNEL_ARGOS:
            l, b = _format_argos(msg_data, embed)
            lines.extend(l)
            custom_buttons.extend(b)
        elif channel_id == CHANNEL_RESTOCKS:
            l, b = _format_restocks_currys(msg_data, embed)
            lines.extend(l)
            custom_buttons.extend(b)
        else:
            # === FALLBACK/GENERIC FORMATTER (Original Logic Refined) ===
            # RETAILER/SOURCE
            retailer = None
            if embed.get("author"):
                retailer = clean_text(embed["author"].get("name"))
            elif tag_info.get("brand"):
                retailer = clean_text(tag_info["brand"])
            
            if retailer and "unknown" not in retailer.lower():
                lines.append(f"üè™ <b>{retailer}</b>")
                lines.append("")
            
            # TITLE
            title = clean_text(embed.get("title") or tag_info.get("product_code") or "Product Alert")
            if tag_info.get("region"): title = f"[{tag_info['region']}] {title}"
            
            lines.append(f"üì¶ <b>{title}</b>")
            lines.append("‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ")
            lines.append("")
            
            # DESC
            if embed.get("description"):
                desc = clean_text(embed["description"])[:400]
                if len(clean_text(embed["description"])) > 400: desc += "..."
                lines.append(desc)
                lines.append("")
            
            # FIELDS
            seen_values = set()
            if embed.get("fields"):
                for field in embed["fields"]:
                    name = clean_text(field.get("name", ""))
                    value = clean_text(field.get("value", ""))
                    
                    if name and value and not any(kw in name.lower() for kw in ['link', 'atc', 'qt', 'checkout', 'offer id']):
                        # Skip N/A values
                        if "n/a" in value.lower(): continue

                        if value in seen_values: continue
                        seen_values.add(value)
                        
                        # Enhanced emoji mapping for comprehensive field coverage
                        name_lower = name.lower()
                        if "status" in name_lower or "stock" in name_lower or "in stock" in name_lower: 
                            icon = "‚úÖ"
                        elif "price" in name_lower or "cost" in name_lower: 
                            icon = "üí∞"
                        elif "type" in name_lower:
                            icon = "üîñ"
                        elif "resell" in name_lower or "profit" in name_lower: 
                            icon = "üìà"
                        elif "member" in name_lower: 
                            icon = "üë•"
                        elif "store" in name_lower or "shop" in name_lower: 
                            icon = "üè™"
                        elif "size" in name_lower: 
                            icon = "üìè"
                        elif "product" in name_lower: 
                            icon = "üì¶"
                        elif "region" in name_lower or "location" in name_lower:
                            icon = "üåç"
                        elif "quantity" in name_lower or "qty" in name_lower:
                            icon = "üî¢"
                        else: 
                            icon = "‚Ä¢"
                        
                        if "price" in name_lower:
                            value = format_price_value(value)
                            lines.append(f"{icon} <b>{name}:</b> <b>{value}</b>")
                        else:
                            lines.append(f"{icon} <b>{name}:</b> {value}")
            lines.append("")
        
        # FOOTER (Common)
        footer = embed.get("footer")
        if footer and "ccn" not in footer.lower() and "monitor" not in footer.lower():
            lines.append(f"‚è∞ {footer}")
        else:
            scraped_time = parse_iso_datetime(msg_data.get("scraped_at", datetime.utcnow().isoformat()))
            lines.append(f"‚è∞ {scraped_time.strftime('%H:%M UTC')}")
            
    else:
        # HUMAN MESSAGE FALLBACK (Refined)
        if plain_content:
            # Don't show generic title if content is short/simple
            if tag_info.get("product_code"): lines.append(f"üì¶ <b>{clean_text(tag_info['product_code'])}</b>")
            
            # Clean up content
            content_display = clean_text(plain_content)
            # If content starts with "Restocks |", bold it or make it a header
            if content_display.lower().startswith("restocks |"):
                parts = content_display.split("|")
                if len(parts) > 1:
                    header = parts[0].strip()
                    body = " | ".join(parts[1:]).strip()
                    lines.append(f"‚ö° <b>{header}</b>")
                    lines.append("")
                    lines.append(body)
                else:
                    lines.append(content_display)
            else:
                 lines.append(content_display[:800])

            scraped_time = parse_iso_datetime(msg_data.get("scraped_at", datetime.utcnow().isoformat()))
            lines.append("")
            lines.append(f"‚è∞ {scraped_time.strftime('%H:%M:%S UTC')}")
    
    text = "\n".join(lines)
    
    # === LINK PARSING (Moved up for image scraping) ===
    all_links = embed.get("links", []) if embed else []
    seen_urls = set()
    title_links = []
    ebay_links = []
    fba_links = []
    atc_links = []
    buy_links = []
    other_links = []
    
    for link in all_links:
        text_lower = link.get('text', '').lower()
        url = link.get('url', '')
        link_text = link.get('text', 'Link')
        field = link.get('field', '').lower()
        link_type = link.get('type', '').lower() # Check for 'title' type
        
        if not url or not url.startswith('http'): continue
        if url in seen_urls: continue
        
        # Helper to categorize
        def add_category(cat_list):
            cat_list.append({'text': link_text, 'url': url, 'field': field})
            seen_urls.add(url)
            
        if link_type == 'title':
            add_category(title_links)
        elif 'atc' in field or 'qt' in field:
            add_category(atc_links)

        elif 'link' in field:
            if any(kw in text_lower for kw in ['sold', 'active', 'google', 'ebay']):
                add_category(ebay_links)
            elif any(kw in text_lower for kw in ['keepa', 'amazon', 'selleramp', 'camel']):
                add_category(fba_links)
            elif any(kw in text_lower for kw in ['buy', 'shop', 'purchase', 'checkout', 'cart']):
                add_category(buy_links)
            else:
                add_category(other_links)
        else:
            if any(kw in text_lower for kw in ['sold', 'active', 'google', 'ebay']):
                add_category(ebay_links)
            elif any(kw in text_lower for kw in ['keepa', 'amazon', 'selleramp', 'camel']):
                add_category(fba_links)
            elif any(kw in text_lower for kw in ['buy', 'shop', 'purchase', 'checkout', 'cart']):
                add_category(buy_links)
            else:
                add_category(other_links)

    # === IMAGE STRATEGY ===
    # Strategy: "Trusted Resolution" vs "Scrape"
    # 1. Get Discord Embed Image and Optimize it.
    # 2. If it's a "Trusted Retailer" (Amazon/eBay) where we KNOW we fixed the resolution -> Use it (Fast).
    # 3. If it's an unknown site (potential low res or logo) -> Scrape Website.
    # 4. Fallback -> Use Discord Image.

    image_url = None
    image_bytes = None
    
    # 1. Get Best Candidate from Discord
    discord_candidate = None
    if embed:
        if embed.get("images"):
            discord_candidate = optimize_image_url(embed["images"][0])
        elif embed.get("thumbnail"):
            discord_candidate = optimize_image_url(embed["thumbnail"])
            
    # 2. Empirical Check: Download and Verify Pixels
    if discord_candidate:
        logger.info(f"   üîç Verifying Discord candidate image: {discord_candidate[:60]}...")
        # Note: download_image_high_quality uses Pillow internally to verify
        downloaded = download_image_high_quality(discord_candidate)
        
        if downloaded:
            try:
                img = Image.open(BytesIO(downloaded))
                width, height = img.size
                
                # Trust it if it's high res (>= 400px in either dimension)
                # This covers long thin images or wide banners correctly
                if width >= 160 or height >= 160:
                    image_url = discord_candidate
                    image_bytes = downloaded
                    logger.info(f"   üì∏ ‚úÖ Discord image is High-Res pixels ({width}x{height}). Skipping scrape.")
                else:
                    logger.info(f"   ‚ö†Ô∏è Discord image is Low-Res pixels ({width}x{height}).")
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Failed to verify Discord image pixels: {e}")

    # 3. Attempt Scraping ONLY if we don't have a high-res candidate yet
    if not image_url:
        target_scrape_url = None
        if title_links: target_scrape_url = title_links[0]['url']
        elif buy_links: target_scrape_url = buy_links[0]['url']
        elif atc_links: target_scrape_url = atc_links[0]['url']
        elif other_links: target_scrape_url = other_links[0]['url']
        
        if target_scrape_url:
            skip_scrape = any(x in target_scrape_url.lower() for x in ['keepa.com', 'ebay.com/sch', 'login', 'cart', 'checkout'])
            if not skip_scrape:
                logger.info(f"   üîç Attempting to scrape images from: {target_scrape_url[:60]}...")
                scraped_images = fetch_product_images(target_scrape_url, max_images=1)
                if scraped_images:
                    scraped_url = scraped_images[0]
                    # Verify scraped image quality as well
                    logger.info(f"   üîç Verifying scraped image: {scraped_url[:60]}...")
                    downloaded_scraped = download_image_high_quality(scraped_url)
                    if downloaded_scraped:
                        image_url = scraped_url
                        image_bytes = downloaded_scraped
                        logger.info(f"   üì∏ ‚úÖ Using verified scraped website image.")

    # 4. Final Fallback (If scraping failed or returned low res, use the best we found)
    if not image_url and discord_candidate:
        image_url = discord_candidate
        # We might already have the bytes from step 2
        # Use them if available, otherwise just use the URL
        logger.info(f"   üì∏ Fallback to Discord image.")
    
    # === BUTTON CREATION ===
    keyboard = []
    
    # Row 1: Price Checking (eBay links)
    if ebay_links:
        row = []
        for link in ebay_links[:3]:
            emoji = 'üí∞' if 'sold' in link['text'].lower() else '‚ö°'
            btn_text = f"{emoji} {link['text'][:15]}"
            row.append(InlineKeyboardButton(btn_text, url=link['url']))
        if row:
            keyboard.append(row)
    
    # Row 2: FBA/Analysis
    if fba_links:
        row = []
        for link in fba_links[:3]:
            emoji = 'üìà' if 'keepa' in link['text'].lower() else 'üîé'
            btn_text = f"{emoji} {link['text'][:15]}"
            row.append(InlineKeyboardButton(btn_text, url=link['url']))
        if row:
            keyboard.append(row)
    
    # Row 3: Direct Buy Links
    if buy_links:
        row = []
        for link in buy_links[:2]:
            btn_text = f"üõí {link['text'][:18]}"
            row.append(InlineKeyboardButton(btn_text, url=link['url']))
        if row:
            keyboard.append(row)
    
    # Row 4: ATC (Add To Cart) Options
    if atc_links:
        row = []
        for link in atc_links[:5]:
            qty_match = re.search(r'\d+', link['text'])
            qty = qty_match.group(0) if qty_match else link['text']
            btn_text = f"üõí {qty}"
            row.append(InlineKeyboardButton(btn_text, url=link['url']))
            if len(row) == 3:
                keyboard.append(row)
                row = []
        if row:
            keyboard.append(row)
    
    # Row 5: Other Links
    if other_links:
        row = []
        for link in other_links[:3]:
            btn_text = f"üîó {link['text'][:15]}"
            row.append(InlineKeyboardButton(btn_text, url=link['url']))
        if row:
            keyboard.append(row)
    
    # Add custom buttons if any
    if custom_buttons:
        keyboard.extend(custom_buttons)
    
    return text, image_url, InlineKeyboardMarkup(keyboard) if keyboard else None, image_bytes



def is_duplicate_source(msg_data: Dict) -> bool:
    """
    Check if message is from a duplicate source (like Profitable Pinger).
    These are filtered out to avoid duplicate alerts.
    """
    raw = msg_data.get("raw_data", {})
    embed = raw.get("embed") or {}
    plain_content = msg_data.get("content", "")
    
    # Check author name
    author_name = (raw.get("author", {}) or {}).get("name", "")
    if author_name and "profitable pinger" in author_name.lower():
        return True
    
    # Check embed author (only if embed exists)
    if embed:
        embed_author = (embed.get("author") or {}).get("name", "")
        if embed_author and "profitable pinger" in embed_author.lower():
            return True
        
        # Check footer (bot signature)
        footer = embed.get("footer", "")
        if footer and "profitable pinger" in footer.lower():
            return True
    
    # Check all fields in embed
    if embed and embed.get("fields"):
        for field in embed["fields"]:
            val = (field.get("value") or "").lower()
            name = (field.get("name") or "").lower()
            if "profitable pinger" in val or "profitable pinger" in name:
                return True
                
    # Check title and description
    if embed:
        title = (embed.get("title") or "").lower()
        desc = (embed.get("description") or "").lower()
        if "profitable pinger" in title or "profitable pinger" in desc:
            return True

    return False


def is_restock_filter_match(msg_data: Dict) -> bool:
    """
    Check if message contains "Just restocked for" phrase.
    These are filtered out based on user request.
    """
    content = (msg_data.get("content") or "").lower()
    if "just restocked for" in content:
        return True
    
    raw = msg_data.get("raw_data", {})
    embed = raw.get("embed") or {}
    
    # Check title and description
    title = (embed.get("title") or "").lower()
    desc = (embed.get("description") or "").lower()
    if "just restocked for" in title or "just restocked for" in desc:
        return True
    
    # Check fields
    if embed.get("fields"):
        for field in embed["fields"]:
            val = (field.get("value") or "").lower()
            name = (field.get("name") or "").lower()
            if "just restocked for" in val or "just restocked for" in name:
                return True
                
    return False


def create_main_menu() -> InlineKeyboardMarkup:
    """Create main menu keyboard"""
    keyboard = [
        [InlineKeyboardButton("üìä My Status", callback_data="status")],
        [InlineKeyboardButton("üîî Toggle Alerts", callback_data="toggle_pause")],
        [InlineKeyboardButton("üéüÔ∏è Redeem Code", callback_data="redeem")],
        [InlineKeyboardButton("‚ùì Help", callback_data="help")]
    ]
    return InlineKeyboardMarkup(keyboard)


def create_menu_with_back(buttons: List[List[InlineKeyboardButton]], back_to: str = "main") -> InlineKeyboardMarkup:
    """Create menu with back button"""
    keyboard = buttons + [
        [InlineKeyboardButton("‚óÄÔ∏è Back", callback_data=f"back:{back_to}")]
    ]
    return InlineKeyboardMarkup(keyboard)


# --- COMMAND HANDLERS ---

async def test_alerts(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    Test command to replay recent N messages.
    Usage: /test [N] (default 1)
    """
    user_id = str(update.effective_user.id)
    if not is_admin(user_id):
        await update.message.reply_text("‚õî Admin only.")
        return

    try:
        count = 1
        if context.args:
            count = int(context.args[0])
            count = min(max(count, 1), 10)  # Clamp between 1 and 10
    except ValueError:
        await update.message.reply_text("usage: /test [number]")
        return

    await update.message.reply_text(f"üîç Fetching last {count} messages...")
    
    # Fetch from Supabase directly
    try:
        headers = {"apikey": poller.supabase_key, "Authorization": f"Bearer {poller.supabase_key}"}
        url = f"{poller.supabase_url}/rest/v1/discord_messages"
        params = {
            "select": "*",
            "order": "scraped_at.desc",
            "limit": count
        }
        
        res = requests.get(url, headers=headers, params=params, timeout=10)
        if res.status_code != 200:
            await update.message.reply_text(f"‚ùå API Error: {res.status_code}")
            return
            
        messages = res.json()
        if not messages:
            await update.message.reply_text("‚ö†Ô∏è No messages found in DB.")
            return

        # Reverse to show oldest first behavior? 
        # Actually usually nice to see latest, but broadcasting sends older first.
        # Let's send in the order fetched (descending) or reverse for chronological replay.
        # Let's do reverse to mimic "stream"
        messages.reverse() 
        
        for msg in messages:
            text, image_url, keyboard, image_bytes = format_telegram_message(msg)
            
            # Prepare photo data once
            photo_data = image_url
            if image_bytes:
                # Reuse pre-verified bytes from formatting step
                photo_data = BytesIO(image_bytes)
                logger.info(f"   ‚úÖ Using pre-verified image bytes ({len(image_bytes)} bytes)")
            elif image_url:
                try:
                    # Fallback for unexpected cases where we have URL but no bytes
                    loop = asyncio.get_event_loop()
                    downloaded = await loop.run_in_executor(sync_executor, download_image_high_quality, image_url)
                    if downloaded:
                        photo_data = BytesIO(downloaded)
                        logger.info(f"   ‚úÖ Processed image via Pillow fallback ({len(downloaded)} bytes)")
                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è Pillow processing failed, falling back to URL: {e}")

            # Send (Admin only)
            if photo_data:
                try:
                    # If it's BytesIO, seek(0) to be safe for multiple users (though not needed here)
                    if isinstance(photo_data, BytesIO): photo_data.seek(0)
                    
                    await context.bot.send_photo(
                        chat_id=user_id,
                        photo=photo_data,
                        caption=text,
                        parse_mode=ParseMode.HTML,
                        reply_markup=keyboard
                    )
                except Exception as e:
                     await context.bot.send_message(
                        chat_id=user_id,
                        text=f"Image failed: {text}",
                        parse_mode=ParseMode.HTML,
                        reply_markup=keyboard
                    )
            else:
                 await context.bot.send_message(
                    chat_id=user_id,
                    text=text,
                    parse_mode=ParseMode.HTML,
                    reply_markup=keyboard
                )
            await asyncio.sleep(0.5)

        await update.message.reply_text("‚úÖ Test complete.")

    except Exception as e:
        logger.error(f"Test error: {e}")
        await update.message.reply_text(f"‚ùå Error: {str(e)}")

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Welcome message with main menu"""
    user_id = str(update.effective_user.id)
    username = update.effective_user.username or update.effective_user.first_name
    
    # Track as potential user if not subscribed
    sm.track_potential_user(user_id, username)
    
    welcome_text = f"""
üëã <b>Welcome to KTTYDROPS!</b>

Hello {username}! Get instant product alerts with all the data you need.

<b>üéØ Features:</b>
‚Ä¢ ‚ö° Real-time notifications
‚Ä¢ üñºÔ∏è Product images
‚Ä¢ üîó Direct action links
‚Ä¢ üìä Full stock & price data
‚Ä¢ ‚è∏Ô∏è Pause/Resume anytime

<b>üìã Status:</b>
"""
    
    if sm.is_active(user_id):
        stats = sm.get_user_stats(user_id)
        welcome_text += f"‚úÖ <b>Active</b> - {stats['days_remaining']} days remaining\n"
        if stats['is_paused']:
            welcome_text += "‚è∏Ô∏è Alerts currently paused\n"
    else:
        welcome_text += "‚ùå <b>Not subscribed</b>\n\nRedeem a code to get started!\n"
    
    await update.message.reply_text(
        welcome_text,
        parse_mode=ParseMode.HTML,
        reply_markup=create_main_menu()
    )

async def button_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Handle inline keyboard button presses with back navigation"""
    query = update.callback_query
    await query.answer()
    
    user_id = str(update.effective_user.id)
    username = update.effective_user.username or update.effective_user.first_name or "User"
    action = query.data
    
    # Handle back button
    if action.startswith("back:"):
        menu_to_go = action.split(":", 1)[1]
        if menu_to_go == "main":
            # Show the same welcome content as /start
            welcome_text = f"""
üëã <b>Welcome to KTTYDROPS!</b>

Hello {username}! Get instant product alerts with all the data you need.

<b>üéØ Features:</b>
‚Ä¢ ‚ö° Real-time notifications
‚Ä¢ üñºÔ∏è Product images
‚Ä¢ üîó Direct action links
‚Ä¢ üìä Full stock & price data
‚Ä¢ ‚è∏Ô∏è Pause/Resume anytime

<b>üìã Status:</b>
"""
            if sm.is_active(user_id):
                stats = sm.get_user_stats(user_id)
                welcome_text += f"‚úÖ <b>Active</b> - {stats['days_remaining']} days remaining\n"
                if stats['is_paused']:
                    welcome_text += "‚è∏Ô∏è Alerts currently paused\n"
            else:
                welcome_text += "‚ùå <b>Not subscribed</b>\n\nRedeem a code to get started!\n"
            
            await query.edit_message_text(welcome_text, parse_mode=ParseMode.HTML, reply_markup=create_main_menu())
        return
    
    if action == "status":
        if not sm.is_active(user_id):
            text = "‚ùå <b>Not Subscribed</b>\n\nUse /start to redeem a code!"
        else:
            stats = sm.get_user_stats(user_id)
            text = f"""
üìä <b>Your Subscription</b>

üë§ User: {stats['username']}
‚è∞ Expires: {stats['expiry_date']}
‚è≥ Days Left: {stats['days_remaining']}
üìÖ Member Since: {stats['days_active']} days

üîî Alerts: {'‚è∏Ô∏è PAUSED' if stats['is_paused'] else '‚úÖ Active'}
"""
        
        buttons = [[InlineKeyboardButton("üîÑ Refresh", callback_data="status")]]
        await query.edit_message_text(text, parse_mode=ParseMode.HTML, reply_markup=create_menu_with_back(buttons, "main"))
    
    elif action == "toggle_pause":
        if not sm.is_active(user_id):
            buttons = []
            await query.edit_message_text("‚ùå You need an active subscription!", parse_mode=ParseMode.HTML, reply_markup=create_menu_with_back(buttons, "main"))
            return
        
        new_state = sm.toggle_pause(user_id)
        status = "‚è∏Ô∏è PAUSED" if new_state else "‚úÖ RESUMED"
        
        text = f"""
{status} <b>Alerts {status}</b>

Your alerts have been {'paused' if new_state else 'resumed'}.
Toggle anytime from the menu.
"""
        buttons = [[InlineKeyboardButton("üìä Status", callback_data="status")]]
        await query.edit_message_text(text, parse_mode=ParseMode.HTML, reply_markup=create_menu_with_back(buttons, "main"))
    
    elif action == "redeem":
        text = """
üéüÔ∏è <b>Redeem Subscription Code</b>

Send your code in this format:
<code>XXXXXXXX</code>

Example: <code>ABC123DEF456</code>

Get a code from your administrator!
"""
        buttons = []
        await query.edit_message_text(text, parse_mode=ParseMode.HTML, reply_markup=create_menu_with_back(buttons, "main"))
    
    elif action == "help":
        text = """
‚ùì <b>Help & Information</b>

<b>How It Works:</b>
1Ô∏è‚É£ Redeem a subscription code
2Ô∏è‚É£ Receive real-time alerts with images & links
3Ô∏è‚É£ Click buttons to check eBay, Keepa, Amazon instantly

<b>Commands:</b>
‚Ä¢ /start - Main menu & status

<b>Tips:</b>
‚Ä¢ Enable Telegram notifications
‚Ä¢ Keep alerts active for drops
‚Ä¢ Use pause when needed

<b>Need Support?</b>
Contact your administrator!
"""
        buttons = []
        await query.edit_message_text(text, parse_mode=ParseMode.HTML, reply_markup=create_menu_with_back(buttons, "main"))


async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Handle code redemption"""
    user_id = str(update.effective_user.id)
    username = update.effective_user.username or update.effective_user.first_name
    text = update.message.text.strip().upper()
    
    code = text.replace("-", "").replace(" ", "")
    
    if len(code) >= 8 and len(code) <= 16:
        if sm.redeem_code(user_id, username, code):
            stats = sm.get_user_stats(user_id)
            response = f"""
üéâ <b>Code Redeemed Successfully!</b>

‚úÖ Subscription Active
‚è∞ Expires: {stats['expiry_date']}
‚è≥ Days: {stats['days_remaining']}

You'll now receive professional alerts!
Use /start for the menu.
"""
            await update.message.reply_text(response, parse_mode=ParseMode.HTML)
        else:
            await update.message.reply_text(
                "‚ùå <b>Invalid Code</b>\n\nCheck your code and try again.",
                parse_mode=ParseMode.HTML
            )
    else:
        await update.message.reply_text(
            "üí° Send a subscription code or use /start for the menu.",
            parse_mode=ParseMode.HTML
        )


async def gen_code(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Admin: Generate codes"""
    if not is_admin(update.effective_user.id): 
        return
    
    try:
        days = int(context.args[0])
        code = sm.generate_code(days)
        await update.message.reply_text(
            f"üîë <b>New Code Generated</b>\n\nCode: <code>{code}</code>\nDuration: {days} days",
            parse_mode=ParseMode.HTML
        )
    except:
        await update.message.reply_text("Usage: /gen <days>")

async def add_bot_admin(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Superadmin: Add a new admin"""
    if not is_superadmin(update.effective_user.id):
        await update.message.reply_text("‚õî Only the superadmin can add other admins.")
        return
    
    if not context.args:
        await update.message.reply_text("Usage: /add_admin <user_id>")
        return
    
    target_id = context.args[0]
    if sm.add_admin(target_id):
        await update.message.reply_text(f"‚úÖ User <code>{target_id}</code> is now an admin.", parse_mode=ParseMode.HTML)
    else:
        await update.message.reply_text(f"‚ùå Could not add user <code>{target_id}</code> as admin. Make sure they have interacted with the bot first.", parse_mode=ParseMode.HTML)

async def remove_bot_admin(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Superadmin: Remove an admin"""
    if not is_superadmin(update.effective_user.id):
        await update.message.reply_text("‚õî Only the superadmin can remove other admins.")
        return
    
    if not context.args:
        await update.message.reply_text("Usage: /remove_admin <user_id>")
        return
    
    target_id = context.args[0]
    if sm.remove_admin(target_id):
        await update.message.reply_text(f"‚úÖ User <code>{target_id}</code> is no longer an admin.", parse_mode=ParseMode.HTML)
    else:
        await update.message.reply_text(f"‚ùå User <code>{target_id}</code> not found or not an admin.", parse_mode=ParseMode.HTML)

async def broadcast_job(context: ContextTypes.DEFAULT_TYPE):
    """
    Poll for new messages and broadcast with timeout protection.
    Uses async lock to prevent overlap and timeout to prevent hanging.
    """
    global job_start_time
    
    # Check if lock is already acquired (job still running)
    if broadcast_lock.locked():
        logger.warning("‚ö†Ô∏è  Previous broadcast job still running - SKIPPING this cycle")
        if job_start_time:
            elapsed = (datetime.utcnow() - job_start_time).total_seconds()
            logger.warning(f"   Job has been running for {elapsed:.1f}s")
        return
    
    # Acquire lock
    async with broadcast_lock:
        job_start_time = datetime.utcnow()
        logger.info(f"üîÑ Broadcast job started at {job_start_time.strftime('%H:%M:%S')}")
        
        try:
            # Wrap the entire job in a timeout
            await asyncio.wait_for(
                _broadcast_job_inner(context),
                timeout=MAX_JOB_RUNTIME
            )
            
            elapsed = (datetime.utcnow() - job_start_time).total_seconds()
            logger.info(f"‚úÖ Broadcast job completed in {elapsed:.1f}s")
            
        except asyncio.TimeoutError:
            elapsed = (datetime.utcnow() - job_start_time).total_seconds()
            logger.error(f"‚ùå Broadcast job TIMEOUT after {elapsed:.1f}s - forcing termination")
            
        except Exception as e:
            elapsed = (datetime.utcnow() - job_start_time).total_seconds()
            err_msg = f"‚ùå <b>Broadcast job error</b> after {elapsed:.1f}s: {type(e).__name__}: {e}"
            logger.error(f"‚ùå Broadcast job error after {elapsed:.1f}s: {type(e).__name__}: {e}")
            logger.error(f"   Full traceback: {traceback.format_exc()}")
            await notify_admins(context, err_msg)
        
        finally:
            job_start_time = None


async def expiry_reminder_job(context: ContextTypes.DEFAULT_TYPE):
    """Notify users with expired subscriptions once every two weeks"""
    expired_uids = sm.get_expired_users_needing_reminder()
    if not expired_uids:
        return

    logger.info(f"‚è∞ EXPIRY REMINDERS: Sending to {len(expired_uids)} user(s)")
    
    reminder_text = """
‚ö†Ô∏è <b>Subscription Expired</b>

Your access to KTTYDROPS has expired. 

To continue receiving real-time notifications with product images and direct links, please redeem a new subscription code.

üéüÔ∏è <b>How to renew:</b>
1. Contact your administrator for a new code.
2. Use the <b>"üéüÔ∏è Redeem Code"</b> button in the main menu.
3. Paste your code to reactivate instantly!

<i>You will receive a reminder every two weeks.</i>
"""
    
    sent = 0
    for uid in expired_uids:
        try:
            await context.bot.send_message(
                chat_id=uid,
                text=reminder_text,
                parse_mode=ParseMode.HTML,
                reply_markup=create_main_menu()
            )
            sm.update_reminder_timestamp(uid)
            sent += 1
            await asyncio.sleep(0.1)  # Small delay between users
        except Exception as e:
            err_str = str(e).lower()
            if "chat not found" in err_str or "bot was blocked" in err_str or "user not found" in err_str:
                logger.warning(f"‚è© Skipping unreachable user {uid} for 14 days ({e})")
                sm.update_reminder_timestamp(uid)
            else:
                logger.error(f"Failed to send expiry reminder to {uid}: {e}")
            
    if sent > 0:
        logger.info(f"‚úÖ Sent {sent} expiry reminder(s)")

async def potential_user_reminder_job(context: ContextTypes.DEFAULT_TYPE):
    """Notify potential users who haven't subscribed once every two weeks"""
    potential_uids = sm.get_potential_users_needing_reminder()
    if not potential_uids:
        return

    logger.info(f"‚è∞ POTENTIAL USER REMINDERS: Sending to {len(potential_uids)} user(s)")
    
    reminder_text = """
üëã <b>Ready to get started?</b>

You recently checked out KTTYDROPS but haven't activated your subscription yet. 

<b>Why subscribe?</b>
‚Ä¢ ‚ö° <b>Instant Alerts:</b> Be the first to know about product drops.
‚Ä¢ üñºÔ∏è <b>Full Data:</b> Images, stock levels, and prices included.
‚Ä¢ üîó <b>Quick Actions:</b> Direct links to eBay, Amazon, and more.

üéüÔ∏è <b>How to get access:</b>
1. Contact your administrator to get a subscription code.
2. Use the <b>"üéüÔ∏è Redeem Code"</b> button in the main menu.
3. Start receiving professional alerts immediately!

<i>You will receive a reminder every two weeks.</i>
"""
    
    sent = 0
    for uid in potential_uids:
        try:
            await context.bot.send_message(
                chat_id=uid,
                text=reminder_text,
                parse_mode=ParseMode.HTML,
                reply_markup=create_main_menu()
            )
            sm.update_potential_reminder_timestamp(uid)
            sent += 1
            await asyncio.sleep(0.1)
        except Exception as e:
            err_str = str(e).lower()
            if "chat not found" in err_str or "bot was blocked" in err_str or "user not found" in err_str:
                logger.warning(f"‚è© Skipping unreachable potential user {uid} for 14 days ({e})")
                sm.update_potential_reminder_timestamp(uid)
            else:
                logger.error(f"Failed to send potential reminder to {uid}: {e}")
            
    if sent > 0:
        logger.info(f"‚úÖ Sent {sent} potential user reminder(s)")


async def _broadcast_job_inner(context: ContextTypes.DEFAULT_TYPE):
    """Inner broadcast logic - separated for timeout handling"""
    
    # Poll for new messages
    try:
        new_msgs = poller.poll_new_messages()
    except Exception as e:
        logger.error(f"‚ùå Failed to poll messages: {e}")
        return
    
    if not new_msgs:
        logger.debug("üî≠ Poll: No new messages found")
        return
    
    # Filter out duplicate sources and unwanted restock alerts
    filtered_msgs = [
        msg for msg in new_msgs 
        if not is_duplicate_source(msg) and not is_restock_filter_match(msg)
    ]
    
    if len(filtered_msgs) < len(new_msgs):
        skipped_count = len(new_msgs) - len(filtered_msgs)
        logger.info(f"üì¨ Poll: Found {len(new_msgs)} message(s), skipped {skipped_count} duplicate source(s)")
    else:
        logger.info(f"üì¨ Poll: Found {len(new_msgs)} new message(s)")
    
    if not filtered_msgs:
        logger.debug("üî≠ No messages after filtering duplicates")
        return
    
    # Get active users
    active_users = sm.get_active_users()
    
    if not active_users:
        logger.warning(f"‚ö†Ô∏è  BROADCAST BLOCKED: No active users!")
        logger.warning(f"   Total users: {len(sm.users)}")
        logger.warning(f"   New messages waiting: {len(filtered_msgs)}")
        return
    
    logger.info(f"üì§ BROADCAST: {len(filtered_msgs)} message(s) ‚Üí {len(active_users)} active user(s)")
    
    # Process messages with batching
    for msg_idx, msg in enumerate(filtered_msgs):
        try:
            logger.debug(f"   üî® Formatting message {msg_idx + 1}/{len(filtered_msgs)}...")
            text, image_url, keyboard, image_bytes = format_telegram_message(msg)
            logger.debug(f"   ‚úì Formatted (text={len(text)} chars, image={'yes' if image_url else 'no'})")
            
            # Validate message is not empty
            if not text or len(text.strip()) == 0:
                continue
            
        except Exception as e:
            logger.error(f"   ‚ùå Failed to format message {msg_idx + 1}: {type(e).__name__}: {e}")
            continue
        
        # Prepare photo data once per message
        photo_data = image_url
        if image_bytes:
            # Reuse pre-verified bytes from formatting step
            photo_data = image_bytes
            logger.info(f"   ‚úÖ Using pre-verified message image bytes ({len(image_bytes)} bytes)")
        elif image_url:
            try:
                # Fallback for unexpected cases
                loop = asyncio.get_event_loop()
                downloaded = await loop.run_in_executor(sync_executor, download_image_high_quality, image_url)
                if downloaded:
                    photo_data = downloaded
                    logger.info(f"   ‚úÖ Processed message image via Pillow fallback ({len(downloaded)} bytes)")
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Pillow processing failed: {e}")

        # Send to all active users with rate limiting
        sent_count = 0
        failed_count = 0
        
        for uid in active_users:
            try:
                # Send with timeout protection
                if photo_data:
                    try:
                        # Prepare the payload (BytesIO if we have raw bytes)
                        current_photo = photo_data
                        if isinstance(photo_data, bytes):
                            current_photo = BytesIO(photo_data)
                            
                        await asyncio.wait_for(
                            context.bot.send_photo(
                                chat_id=uid,
                                photo=current_photo,
                                caption=text[:1024],
                                parse_mode=ParseMode.HTML,
                                reply_markup=keyboard
                            ),
                            timeout=12.0
                        )
                        sent_count += 1
                        
                    except asyncio.TimeoutError:
                        logger.warning(f"   ‚è±Ô∏è  {uid}: Photo send timeout")
                        # Fallback to text
                        await asyncio.wait_for(
                            context.bot.send_message(
                                chat_id=uid,
                                text=text,
                                parse_mode=ParseMode.HTML,
                                reply_markup=keyboard,
                                disable_web_page_preview=False
                            ),
                            timeout=10.0
                        )
                        sent_count += 1
                        
                    except Exception as photo_error:
                        logger.error(f"   ‚ùå {uid}: Photo failed - {type(photo_error).__name__}")
                        # Fallback to text
                        try:
                            await asyncio.wait_for(
                                context.bot.send_message(
                                    chat_id=uid,
                                    text=text,
                                    parse_mode=ParseMode.HTML,
                                    reply_markup=keyboard,
                                    disable_web_page_preview=False
                                ),
                                timeout=10.0
                            )
                            sent_count += 1
                        except:
                            failed_count += 1
                else:
                    # Text-only
                    await asyncio.wait_for(
                        context.bot.send_message(
                            chat_id=uid,
                            text=text,
                            parse_mode=ParseMode.HTML,
                            reply_markup=keyboard,
                            disable_web_page_preview=False
                        ),
                        timeout=10.0
                    )
                    sent_count += 1
            
            except asyncio.TimeoutError:
                logger.warning(f"   ‚è±Ô∏è  {uid}: Message send timeout")
                failed_count += 1
                
            except Exception as e:
                error_str = str(e)
                if "user not found" in error_str.lower() or "chat_id_invalid" in error_str.lower():
                    logger.warning(f"   ‚õî {uid}: User invalid/blocked")
                elif "bot was blocked" in error_str.lower():
                    logger.warning(f"   üö´ {uid}: Bot blocked by user")
                elif "badrequest" in error_str.lower():
                    # Log full error for BadRequest to diagnose formatting issues
                    logger.error(f"   ‚ùå {uid}: BadRequest - {error_str}")
                    logger.error(f"      Message preview: {text[:200]}...")
                else:
                    logger.error(f"   ‚ùå {uid}: {type(e).__name__}: {error_str}")
                failed_count += 1
            
            # Rate limit protection - small delay between users
            await asyncio.sleep(0.05)
        
        logger.info(f"   üìä Message {msg_idx + 1}: ‚úÖ {sent_count} sent, ‚ùå {failed_count} failed")
        
        # Update cursor ONLY after successful processing of this message
        # This ensures failed messages are retried on next poll
        if sent_count > 0:  # At least one user received it
            msg_scraped_at = msg.get("scraped_at")
            if msg_scraped_at:
                poller.update_cursor(msg_scraped_at, msg)
                logger.debug(f"   üìå Cursor updated to: {msg_scraped_at}")

# 4. COMMAND MENU SETUP

# Default commands for all users
DEFAULT_COMMANDS = [
    BotCommand("start", "Start the bot and see main menu"),
    BotCommand("help", "Show available commands"),
]

# Additional commands for admins
ADMIN_COMMANDS = [
    BotCommand("start", "Start the bot and see main menu"),
    BotCommand("help", "Show available commands"),
    BotCommand("gen", "Generate subscription code (e.g., /gen 30)"),
    BotCommand("test", "Test recent alerts (e.g., /test 5)"),
]

# Additional commands for superadmin
SUPERADMIN_COMMANDS = [
    BotCommand("start", "Start the bot and see main menu"),
    BotCommand("help", "Show available commands"),
    BotCommand("gen", "Generate subscription code (e.g., /gen 30)"),
    BotCommand("test", "Test recent alerts (e.g., /test 5)"),
    BotCommand("add_admin", "Add a user as admin (e.g., /add_admin 123456)"),
    BotCommand("remove_admin", "Remove admin status (e.g., /remove_admin 123456)"),
]

async def setup_bot_commands(application: Application) -> None:
    """Set up command menus for different user roles"""
    try:
        # Set default commands for all users
        await application.bot.set_my_commands(DEFAULT_COMMANDS, scope=BotCommandScopeDefault())
        logger.info("   ‚úÖ Default command menu set")
        
        # Set admin commands for secondary admins
        for user_id, user_data in sm.users.items():
            if user_data.get("is_admin", False):
                try:
                    await application.bot.set_my_commands(
                        ADMIN_COMMANDS, 
                        scope=BotCommandScopeChat(chat_id=int(user_id))
                    )
                    logger.info(f"   ‚úÖ Admin menu set for user {user_id}")
                except Exception as e:
                    logger.debug(f"   Could not set admin menu for {user_id}: {e}")
        
        # Set superadmin commands
        if ADMIN_USER_ID:
            try:
                await application.bot.set_my_commands(
                    SUPERADMIN_COMMANDS, 
                    scope=BotCommandScopeChat(chat_id=int(ADMIN_USER_ID))
                )
                logger.info(f"   ‚úÖ Superadmin menu set for {ADMIN_USER_ID}")
            except Exception as e:
                logger.debug(f"   Could not set superadmin menu: {e}")
                
    except Exception as e:
        logger.error(f"   ‚ùå Failed to set command menus: {e}")

async def show_help(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Show available commands based on user role"""
    user_id = str(update.effective_user.id)
    
    # Build menu based on role
    if is_superadmin(user_id):
        menu_text = """
üéõÔ∏è <b>Superadmin Commands</b>

<b>General:</b>
/start - Main menu & status
/help - Show this command list

<b>Admin Tools:</b>
/gen [days] - Generate subscription code
/test [count] - Test recent alerts

<b>User Management:</b>
/add_admin [user_id] - Add an admin
/remove_admin [user_id] - Remove an admin
"""
    elif is_admin(user_id):
        menu_text = """
üéõÔ∏è <b>Admin Commands</b>

<b>General:</b>
/start - Main menu & status
/help - Show this command list

<b>Admin Tools:</b>
/gen [days] - Generate subscription code
/test [count] - Test recent alerts
"""
    else:
        menu_text = """
üìã <b>Available Commands</b>

/start - Main menu & status
/help - Show this command list

Use the menu buttons for more options!
"""
    
    if update.message:
        await update.message.reply_text(menu_text, parse_mode=ParseMode.HTML)
    elif update.callback_query:
        await update.callback_query.message.reply_text(menu_text, parse_mode=ParseMode.HTML)

# 5. RUN BOT FUNCTION
def run_bot():
    """Run bot with professional alert system"""
    try:
        if not TELEGRAM_TOKEN:
            logger.error("‚ùå TELEGRAM_TOKEN not set!")
            return
        
        logger.info("\n" + "=" * 80)
        logger.info("üöÄ TELEGRAM BOT INITIALIZATION")
        logger.info("=" * 80)
        logger.info(f"   Token: {TELEGRAM_TOKEN[:15]}...***{TELEGRAM_TOKEN[-5:]}")
        logger.info(f"   Admin ID: {ADMIN_USER_ID}")
        logger.info(f"   Poll Interval: {POLL_INTERVAL} seconds")
        logger.info(f"   Max Runtime: {MAX_JOB_RUNTIME} seconds")
        
        app = Application.builder().token(TELEGRAM_TOKEN).post_init(setup_bot_commands).build()
        
        # Command Handlers
        app.add_handler(CommandHandler("start", start))
        app.add_handler(CommandHandler("gen", gen_code))
        app.add_handler(CommandHandler("add_admin", add_bot_admin))
        app.add_handler(CommandHandler("remove_admin", remove_bot_admin))
        app.add_handler(CommandHandler("test", test_alerts))
        app.add_handler(CommandHandler("help", show_help))  # Help command
        app.add_handler(CallbackQueryHandler(button_handler))
        app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
        
        if app.job_queue:
            logger.info("   Adding broadcast job with overlap protection...")
            app.job_queue.run_repeating(
                broadcast_job, 
                interval=POLL_INTERVAL, 
                first=10
            )

            # Register expiry reminders (Check daily)
            logger.info("   Adding bi-weekly expiry reminder job...")
            app.job_queue.run_repeating(
                expiry_reminder_job,
                interval=86400, # Once a day
                first=30        # Start after 30 seconds
            )

            # Register potential user reminders (Check daily)
            logger.info("   Adding bi-weekly potential user reminder job...")
            app.job_queue.run_repeating(
                potential_user_reminder_job,
                interval=86400, # Once a day
                first=60        # Start after 60 seconds
            )
            
            logger.info(f"   ‚úÖ Job queue running (poll every {POLL_INTERVAL}s)")
        
        # Show active users count on startup
        active_count = len(sm.get_active_users())
        total_count = len(sm.users)
        logger.info(f"   üìä Users: {total_count} total, {active_count} active")
        logger.info("=" * 80 + "\n")
        
        logger.info("üì° Starting polling loop...")
        app.run_polling(allowed_updates=Update.ALL_TYPES, stop_signals=[])
        
    except KeyboardInterrupt:
        logger.info("‚ö†Ô∏è  Bot interrupted by user")
    except Exception as e:
        logger.error(f"‚ùå CRITICAL BOT ERROR: {e}")
        logger.error(traceback.format_exc())
